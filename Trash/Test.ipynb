{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Librerias*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Generador de palabras, data sintetica, y API GPT__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre1, sintiéndose feliz, dice que el Tema1 es muy aburrido, tiene muchas esperanzas para el futuro.\n",
      "Nombre2 está triste porque el Tema1 es muy aburrido, no puede esperar a ver qué sucede a continuación.\n",
      "Nombre2 está triste porque el Tema2 es muy frustrante, tiene muchas esperanzas para el futuro.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Aquí van los nombres, temas y emociones que se usarán para generar los tweets\n",
    "nombres = ['Nombre1', 'Nombre2', 'Nombre3']\n",
    "temas = ['Tema1', 'Tema2', 'Tema3']\n",
    "emociones = ['feliz', 'triste', 'enojado']\n",
    "\n",
    "def genera_tweet(nombre, tema, emocion):\n",
    "    frases_inicio = [\n",
    "        f\"{nombre} está {emocion} porque\",\n",
    "        f\"A {nombre} le parece {emocion} que\",\n",
    "        f\"{nombre}, sintiéndose {emocion}, dice que\"\n",
    "    ]\n",
    "\n",
    "    frases_medio = [\n",
    "        f\"el {tema} es realmente emocionante\",\n",
    "        f\"el {tema} es muy frustrante\",\n",
    "        f\"el {tema} es muy aburrido\",\n",
    "    ]\n",
    "\n",
    "    frases_final = [\n",
    "        \"no puede esperar a ver qué sucede a continuación.\",\n",
    "        \"se siente muy decepcionado.\",\n",
    "        \"tiene muchas esperanzas para el futuro.\"\n",
    "    ]\n",
    "\n",
    "    tweet = random.choice(frases_inicio) + ' ' + random.choice(frases_medio) + ', ' + random.choice(frases_final)\n",
    "    return tweet\n",
    "\n",
    "# Generar 3 tweets\n",
    "for i in range(3):\n",
    "    print(genera_tweet(random.choice(nombres), random.choice(temas), random.choice(emociones)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert \"openai\" in openai_secret_manager.get_services()\n",
    "secrets = openai_secret_manager.get_secret(\"openai\")\n",
    "\n",
    "import openai\n",
    "openai.api_key = secrets[\"api_key\"]\n",
    "\n",
    "def generate_tweet(name: str, topic: str, prompt: str):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"davinci\",\n",
    "        prompt=f\"Generate a tweet for {name} about {topic}. {prompt}\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=280,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return response['choices'][0]['text']\n",
    "\n",
    "# Example usage\n",
    "name = \"John Doe\"\n",
    "topic = \"Climate Change\"\n",
    "prompt = \"John is passionate about reducing carbon emissions.\"\n",
    "tweet = generate_tweet(name, topic, prompt)\n",
    "print(tweet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROGRAMA SEPARADOS Y FABRICADOR DE CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo CSV ha sido creado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Definir el texto\n",
    "texto = ''' Carlos Hernandez, Qué emoción al ver cómo la tecnología 5G revoluciona la forma en que interactuamos con el mundo. ¡El futuro es ahora! #\n",
    "Carlos Hernandez, Los algoritmos de recomendación de algunos servicios de streaming podrían mejorar mucho. ¡Es como si no me conocieran! #\n",
    "Carlos Hernandez, Actualizando mi PC. Nada más emocionante que darle un poco de vida nueva a una vieja máquina. #\n",
    "Carlos Hernandez, La obsolescencia programada es una práctica que debe terminarse. Todos deberíamos poder usar nuestros dispositivos por más tiempo. #\n",
    "Carlos Hernandez, Aprendiendo más sobre inteligencia artificial. Impresionante cómo las máquinas pueden aprender y adaptarse. #\n",
    "Carlos Hernandez, A veces es desalentador ver cuánta desinformación hay sobre la ciberseguridad. #\n",
    "Carlos Hernandez, La realidad virtual no deja de asombrarme. Las posibilidades son infinitas. #\n",
    "Carlos Hernandez, Es triste ver cómo algunas compañías de tecnología no toman en serio la privacidad del usuario. #\n",
    "Carlos Hernandez, Probando el nuevo smartphone. A ver si cumple con las expectativas. #\n",
    "Carlos Hernandez, Agotado de constantes actualizaciones de software que parecen más bugs que mejoras. #\n",
    "Carlos Hernandez, La tecnología LED está cambiando el juego en la iluminación. ¡Adiós a las bombillas incandescentes! #\n",
    "Carlos Hernandez, Pensando en la cantidad de cables que hay en mi escritorio. ¿Alguien más sueña con un mundo completamente inalámbrico? #\n",
    "Carlos Hernandez, La internet de las cosas promete, pero necesitamos estándares de seguridad más fuertes. #\n",
    "Carlos Hernandez, Frustrado con la velocidad de internet. ¿Es demasiado pedir un día sin problemas de conexión? #\n",
    "Carlos Hernandez, Descubriendo las maravillas de la carga rápida. Ahora si se puede decir \"en un abrir y cerrar de ojos\". #\n",
    "Carlos Hernandez, Realmente molesto con la falta de transparencia en cuanto a la recopilación de datos. #\n",
    "Carlos Hernandez, Creando mi primera aplicación. Es un desafío, pero estoy emocionado. #\n",
    "Carlos Hernandez, Extrañando los días en que los teléfonos venían con auriculares incluidos. #\n",
    "Carlos Hernandez, Aprendiendo sobre tecnologías verdes. Es hora de hacer más sostenible nuestro mundo. #\n",
    "Carlos Hernandez, Desearía que los desarrolladores consideraran más a los usuarios finales al diseñar sus productos. #\n",
    "Carlos Hernandez, Aprecio la belleza del código abierto. Creando juntos, avanzamos más rápido. #\n",
    "Carlos Hernandez, Lamentable que el ciberacoso sea aún un problema tan prevalente. Necesitamos más herramientas para combatirlo. #\n",
    "Carlos Hernandez, Recordando los viejos tiempos con los disquetes. ¡Cómo ha avanzado el almacenamiento de datos! #\n",
    "Carlos Hernandez, Días en los que te das cuenta de que la inteligencia artificial todavía tiene mucho que aprender. #\n",
    "Carlos Hernandez, Leyendo sobre blockchain. Realmente es una revolución en el mundo digital. #\n",
    "Carlos Hernandez, Nada más frustrante que una pantalla táctil que no responde. #'\n",
    "'''\n",
    "# Dividir el texto en oraciones\n",
    "oraciones = texto.split('#')\n",
    "\n",
    "# Dividir el texto en oraciones\n",
    "oraciones = texto.split('#')\n",
    "\n",
    "# Crear el archivo csv y escribir cada oración en una nueva línea\n",
    "with open('mi_archivo.csv', 'w', newline='' ,encoding='utf-8') as archivo:\n",
    "    escritor = csv.writer(archivo)\n",
    "    for oracion in oraciones:\n",
    "        # Removemos los espacios en blanco antes y después de la oración y solo escribimos si hay texto.\n",
    "        oracion = oracion.strip()\n",
    "        if oracion:\n",
    "            escritor.writerow([oracion])\n",
    "\n",
    "# Leer el contenido del archivo csv y reemplazar las comillas\n",
    "with open('mi_archivo.csv', 'r', encoding='utf-8') as archivo:\n",
    "    contenido = archivo.read()\n",
    "\n",
    "# Eliminar las comillas\n",
    "contenido = contenido.replace('\"', '')\n",
    "\n",
    "# Sobrescribir el archivo csv con el contenido sin comillas\n",
    "with open('mi_archivo.csv', 'w', encoding='utf-8') as archivo:\n",
    "    archivo.write(contenido)\n",
    "\n",
    "print('El archivo CSV ha sido creado exitosamente.')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de clasificacion de emociones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import openai_secret_manager\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "skiprows = [14, 18, 22]  # Lista de números de línea a omitir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('xxx', skiprows=skiprows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['texto'], data['emocion'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test_vectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión del modelo:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELO QUE DEFINA EL TEMA DE LA CUENTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\matia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de topics: ['sueña', 'leyendo', 'ciberseguridad', 'ahora']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import langid\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Instalar e importar las palabras de detención para varios idiomas\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = {\n",
    "    'en': set(stopwords.words('english')),\n",
    "    'es': set(stopwords.words('spanish')),\n",
    "    'fr': set(stopwords.words('french')),\n",
    "    'it': set(stopwords.words('italian')),\n",
    "    'de': set(stopwords.words('german'))\n",
    "}\n",
    "\n",
    "# Leer el archivo csv\n",
    "data = pd.read_csv('csv\\carlos_hernandez_tweets.csv')\n",
    "\n",
    "# Inicializar Spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    # Preprocesamiento con Spacy\n",
    "    doc = nlp(tweet.lower())\n",
    "    \n",
    "    # Detectar el idioma del tweet\n",
    "    lang = langid.classify(tweet)[0]\n",
    "\n",
    "    # Tokenización\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.lemma_ not in stop_words.get(lang, set())]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Procesar todos los tweets\n",
    "data['processed_tweets'] = data['tweet'].apply(process_tweet)\n",
    "\n",
    "# Crear un diccionario con Gensim\n",
    "dictionary = Dictionary(data['processed_tweets'])\n",
    "\n",
    "# Crear una representación de bolsa de palabras (BoW)\n",
    "corpus = [dictionary.doc2bow(tweet) for tweet in data['processed_tweets']]\n",
    "\n",
    "# Entrenar el modelo LDA\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, random_state=0)\n",
    "\n",
    "# Obtener los nombres de los topics más importantes\n",
    "topics = lda_model.print_topics(num_topics=5, num_words=10)\n",
    "\n",
    "# Filtrar las palabras de apoyo y obtener la lista de temas sin esas palabras\n",
    "topic_list = []\n",
    "for topic in topics:\n",
    "    words = topic[1].split('*')[-1].strip().replace('\"', '').split()\n",
    "    filtered_words = [word for word in words if len(word) > 2]\n",
    "    if filtered_words:\n",
    "        topic_list.append(filtered_words[0])\n",
    "\n",
    "# Eliminar palabras no deseadas\n",
    "topic_list = [topic for topic in topic_list if len(topic) > 2]\n",
    "\n",
    "# Imprimir la lista de temas\n",
    "print(\"Lista de topics:\", topic_list[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de topics: ['cantidad', 'hora', 'pedir', 'desear', 'obsolescencia']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Leer el archivo csv\n",
    "data = pd.read_csv('csv\\carlos_hernandez_tweets.csv')\n",
    "\n",
    "# Inicializar Spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    # Preprocesamiento con Spacy\n",
    "    doc = nlp(tweet.lower())\n",
    "    \n",
    "    # Tokenización\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Procesar todos los tweets\n",
    "data['processed_tweets'] = data['tweet'].apply(process_tweet)\n",
    "\n",
    "# Crear un diccionario con Gensim\n",
    "dictionary = Dictionary(data['processed_tweets'])\n",
    "\n",
    "# Crear una representación de bolsa de palabras (BoW)\n",
    "corpus = [dictionary.doc2bow(tweet) for tweet in data['processed_tweets']]\n",
    "\n",
    "# Entrenar el modelo LDA\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, random_state=0)\n",
    "\n",
    "# Obtener los temas principales\n",
    "topics = lda_model.print_topics(num_topics=5, num_words=10)\n",
    "\n",
    "# Extraer los nombres de los temas\n",
    "topic_list = []\n",
    "for topic in topics:\n",
    "    words = topic[1].split('*')[-1].strip().replace('\"', '').split()\n",
    "    filtered_words = [word for word in words if len(word) > 2]\n",
    "    if filtered_words:\n",
    "        topic_list.append(filtered_words[0])\n",
    "\n",
    "# Imprimir la lista de temas\n",
    "print(\"Lista de topics:\", topic_list[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Descargar los datos necesarios de NLTK\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Inicializar el lematizador y obtener las stop words del inglés\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(text):\n",
    "    # Convertir el texto a minúsculas\n",
    "    text = text.lower()\n",
    "    # Eliminar caracteres especiales\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Eliminar stop words y lematizar\n",
    "    text = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    return text\n",
    "\n",
    "# Aplicar el preprocesamiento al texto\n",
    "data['text'] = data['text'].apply(preprocess)\n",
    "\n",
    "# Continuar con el resto del código aquí...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deteccion de emociones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  user                                              tweet   emotion\n",
      "0  Ari  ¡A veces la productividad puede ser abrumadora...  optimism\n",
      "1  Ari  ¡La procrastinación es uno de los peores enemi...       joy\n",
      "2  Ari  \"A veces la frustración es parte de la product...       joy\n",
      "3  Ari  Sentirse desalentado y sin motivación es parte...     anger\n",
      "4  Ari  A veces es difícil ser productivo cuando nos s...     anger\n"
     ]
    }
   ],
   "source": [
    "# Primero, importamos las librerías necesarias\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Cargamos el dataframe\n",
    "df = pd.read_csv('csv\\Ari_tweets.csv')\n",
    "\n",
    "# Cargamos el modelo de detección de sentimientos\n",
    "nlp = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-emotion')\n",
    "\n",
    "# Aplicamos el modelo a cada tweet\n",
    "df['emotion'] = df['tweet'].apply(lambda x: nlp(x)[0]['label'])\n",
    "\n",
    "# Imprimimos los primeros registros para verificar\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Cargamos el dataframe\n",
    "df = pd.read_csv('csv\\Mati_tweets.csv')\n",
    "\n",
    "# Función para limpiar el texto del tweet\n",
    "def clean_text(text):\n",
    "    # Eliminamos las menciones, los enlaces y los caracteres especiales\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # Eliminamos las menciones\n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+', '', text) # Eliminamos los enlaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9ñáéíóúüÁÉÍÓÚÜ\\s]', '', text) # Eliminamos los caracteres especiales\n",
    "    return text\n",
    "\n",
    "# Función para traducir el texto al inglés y luego analizar la emoción\n",
    "def detect_emotion(text):\n",
    "    # Limpiamos el texto\n",
    "    text_cleaned = clean_text(text)\n",
    "    #si el texto no esta en español que haga esto\n",
    "    if not langid.classify(text_cleaned)[0] == 'es':\n",
    "        text_cleaned = GoogleTranslator(source='auto', target='spanish').translate(text_cleaned)\n",
    "    else:\n",
    "        text_cleaned = text_cleaned\n",
    "    # Analizamos la emoción en el texto traducido\n",
    "    emotion = nlp(text_cleaned)[0]['label']\n",
    "    return emotion\n",
    "\n",
    "# Cargamos el modelo de detección de emociones\n",
    "nlp = pipeline('sentiment-analysis', model='finiteautomata/beto-emotion-analysis')\n",
    "\n",
    "# Aplicamos la función a cada tweet\n",
    "df['emotion'] = df['tweet'].apply(detect_emotion)\n",
    "\n",
    "# Imprimimos los primeros registros para verificar\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Sad news for those seeking joy: the search for...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Devastating news of more job losses today, but...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Sad news today, but we must keep a positive at...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Sad news: another tragedy has hit the world to...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Sometimes it's hard to find inspiration when s...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Sometimes it can be hard to find motivation an...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mati</td>\n",
       "      <td>The path to inspiration can be long and windin...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Sometimes it feels like no matter what you do,...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Sometimes it can be hard to find the motivatio...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Sometimes it feels like inspiration has abando...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Sometimes it feels like inspiration is so far ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Sometimes it's hard to find inspiration when f...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Sometimes it's hard to find inspiration when y...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Sometimes it's hard to find inspiration when y...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Sometimes it feels like we're stuck in a rut, ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Sometimes life can be overwhelming and it can ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Sometimes it's hard to find inspiration when y...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Sometimes it can be difficult to find the moti...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Mati</td>\n",
       "      <td>Sometimes we all need a little inspiration to ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user                                              tweet  emotion\n",
       "0   Mati  Sad news for those seeking joy: the search for...  sadness\n",
       "1   Mati  Devastating news of more job losses today, but...  sadness\n",
       "2   Mati  Sad news today, but we must keep a positive at...    anger\n",
       "3   Mati  Sad news: another tragedy has hit the world to...  sadness\n",
       "4   Mati  Sometimes it's hard to find inspiration when s...  sadness\n",
       "5   Mati  Sometimes it can be hard to find motivation an...    anger\n",
       "6   Mati  The path to inspiration can be long and windin...    anger\n",
       "7   Mati  Sometimes it feels like no matter what you do,...  sadness\n",
       "8   Mati  Sometimes it can be hard to find the motivatio...    anger\n",
       "9   Mati  Sometimes it feels like inspiration has abando...  sadness\n",
       "10  Mati  Sometimes it feels like inspiration is so far ...    anger\n",
       "11  Mati  Sometimes it's hard to find inspiration when f...  sadness\n",
       "12  Mati  Sometimes it's hard to find inspiration when y...  sadness\n",
       "13  Mati  Sometimes it's hard to find inspiration when y...    anger\n",
       "14  Mati  Sometimes it feels like we're stuck in a rut, ...    anger\n",
       "15  Mati  Sometimes life can be overwhelming and it can ...    anger\n",
       "16  Mati  Sometimes it's hard to find inspiration when y...  sadness\n",
       "17  Mati  Sometimes it can be difficult to find the moti...    anger\n",
       "18  Mati  Sometimes we all need a little inspiration to ...  sadness"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def identificar_(dataframe):\n",
    "\n",
    "    modelo = \"jirmauritz/bert-multilingual-emoji\"\n",
    "\n",
    "\n",
    "    clasificador_emociones = pipeline(\"sentiment-analysis\", model=modelo)\n",
    "    emotions_list = []\n",
    "    for texto in dataframe['tweet']:\n",
    "        resultado = clasificador_emociones(texto)\n",
    "        emotions = resultado[0]['label']\n",
    "        emotions_list.append(emotions)\n",
    "    dataframe['emotion'] = emotions_list\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identificar_emociones(dataframe):\n",
    "\n",
    "    if idioma == \"Español\" or \"deutch\" or \"francés\" or \"english\" or \"italiano\":\n",
    "        modelo = \"MilaNLProc/xlm-emo-t\"\n",
    "\n",
    "    clasificador_emotion = pipeline(\"sentiment-analysis\", model=modelo)\n",
    "    emociones = []\n",
    "    for texto in dataframe['tweet']:\n",
    "        resultado = clasificador_emotion(texto)\n",
    "        etiqueta = resultado[0]['label']\n",
    "        emociones.append(etiqueta)\n",
    "    dataframe['sentimiento'] = emociones\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..\\csv\\Ari_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AlfaRomeo</td>\n",
       "      <td>La vie est courte, profitez-en pour trouver de...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AlfaRomeo</td>\n",
       "      <td>Même si les relations et les rencontres peuven...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AlfaRomeo</td>\n",
       "      <td>Lorsqu'on parle de relations et de rencontres,...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AlfaRomeo</td>\n",
       "      <td>La vie est courte, profitez de chaque moment a...</td>\n",
       "      <td>optimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AlfaRomeo</td>\n",
       "      <td>Laissons les relations et les rencontres s'épa...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>AlfaRomeo</td>\n",
       "      <td>Être en paix avec soi-même est la base d'une r...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>AlfaRomeo</td>\n",
       "      <td>Les relations et les rencontres sont parfois d...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>AlfaRomeo</td>\n",
       "      <td>Que la paz soit au rendez-vous de vos relation...</td>\n",
       "      <td>optimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>AlfaRomeo</td>\n",
       "      <td>Même si la passion est apréciée dans les relat...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>AlfaRomeo</td>\n",
       "      <td>A la recherche d'amour, de paix et d'équilibre...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user                                              tweet   emotion\n",
       "0   AlfaRomeo  La vie est courte, profitez-en pour trouver de...     anger\n",
       "1   AlfaRomeo  Même si les relations et les rencontres peuven...     anger\n",
       "2   AlfaRomeo  Lorsqu'on parle de relations et de rencontres,...     anger\n",
       "3   AlfaRomeo  La vie est courte, profitez de chaque moment a...  optimism\n",
       "4   AlfaRomeo  Laissons les relations et les rencontres s'épa...       joy\n",
       "..        ...                                                ...       ...\n",
       "95  AlfaRomeo  Être en paix avec soi-même est la base d'une r...     anger\n",
       "96  AlfaRomeo  Les relations et les rencontres sont parfois d...     anger\n",
       "97  AlfaRomeo  Que la paz soit au rendez-vous de vos relation...  optimism\n",
       "98  AlfaRomeo  Même si la passion est apréciée dans les relat...     anger\n",
       "99  AlfaRomeo  A la recherche d'amour, de paix et d'équilibre...     anger\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (C:/Users/matia/.cache/huggingface/datasets/dair-ai___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n",
      "100%|██████████| 3/3 [00:00<00:00, 603.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"dair-ai/emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>i just had a very brief time in the beanbag an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>i am now turning and i feel pathetic that i am...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>i feel strong and good overall</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>i feel like this was such a rude comment and i...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>i know a lot but i feel so stupid because i ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0                                i didnt feel humiliated      0\n",
       "1      i can go from feeling so hopeless to so damned...      0\n",
       "2       im grabbing a minute to post i feel greedy wrong      3\n",
       "3      i am ever feeling nostalgic about the fireplac...      2\n",
       "4                                   i am feeling grouchy      3\n",
       "...                                                  ...    ...\n",
       "15995  i just had a very brief time in the beanbag an...      0\n",
       "15996  i am now turning and i feel pathetic that i am...      0\n",
       "15997                     i feel strong and good overall      1\n",
       "15998  i feel like this was such a rude comment and i...      3\n",
       "15999  i know a lot but i feel so stupid because i ca...      0\n",
       "\n",
       "[16000 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.DataFrame(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(dataset['test'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Importing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando TensorFlow versión 2.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (C:/Users/matia/.cache/huggingface/datasets/dair-ai___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n",
      "100%|██████████| 3/3 [00:00<00:00, 375.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5}\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 16)            160000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 50, 40)           5920      \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 40)               9760      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 246       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,926\n",
      "Trainable params: 175,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "500/500 [==============================] - 18s 24ms/step - loss: 1.2757 - accuracy: 0.5123 - val_loss: 0.8196 - val_accuracy: 0.7145\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 12s 24ms/step - loss: 0.5587 - accuracy: 0.8081 - val_loss: 0.6146 - val_accuracy: 0.7925\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 12s 23ms/step - loss: 0.3705 - accuracy: 0.8742 - val_loss: 0.5439 - val_accuracy: 0.8185\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 13s 25ms/step - loss: 0.2811 - accuracy: 0.9030 - val_loss: 0.5241 - val_accuracy: 0.8305\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 12s 24ms/step - loss: 0.2003 - accuracy: 0.9346 - val_loss: 0.4494 - val_accuracy: 0.8615\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 13s 25ms/step - loss: 0.1506 - accuracy: 0.9546 - val_loss: 0.4588 - val_accuracy: 0.8730\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 13s 26ms/step - loss: 0.1244 - accuracy: 0.9629 - val_loss: 0.4379 - val_accuracy: 0.8835\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 13s 26ms/step - loss: 0.1012 - accuracy: 0.9696 - val_loss: 0.4016 - val_accuracy: 0.8850\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 13s 26ms/step - loss: 0.0907 - accuracy: 0.9724 - val_loss: 0.4304 - val_accuracy: 0.8855\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.0778 - accuracy: 0.9768 - val_loss: 0.4338 - val_accuracy: 0.8910\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.0743 - accuracy: 0.9778 - val_loss: 0.4226 - val_accuracy: 0.8895\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.0646 - accuracy: 0.9811 - val_loss: 0.4299 - val_accuracy: 0.8915\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.0512 - accuracy: 0.9836 - val_loss: 0.4256 - val_accuracy: 0.8960\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 13s 25ms/step - loss: 0.0515 - accuracy: 0.9843 - val_loss: 0.4779 - val_accuracy: 0.8935\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 13s 25ms/step - loss: 0.0444 - accuracy: 0.9868 - val_loss: 0.4611 - val_accuracy: 0.8965\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 13s 27ms/step - loss: 0.0372 - accuracy: 0.9883 - val_loss: 0.5820 - val_accuracy: 0.8900\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 13s 27ms/step - loss: 0.0440 - accuracy: 0.9858 - val_loss: 0.5048 - val_accuracy: 0.8945\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.5203 - accuracy: 0.8880\n",
      "Frase: im not sure if im more at peace with our situation or if im just not feeling as bitter about it but in the past five months something has changed within me\n",
      "Emoción: 3\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Emoción Predicha: 3\n",
      "63/63 [==============================] - 1s 9ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def show_history(h):\n",
    "    epochs_trained = len(h.history['loss'])\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(epochs_trained), h.history['accuracy'], label='Training')\n",
    "    plt.plot(range(epochs_trained), h.history['val_accuracy'], label='Validation')\n",
    "    plt.ylim([0., 1.])\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(epochs_trained), h.history['loss'], label='Training')\n",
    "    plt.plot(range(epochs_trained), h.history['val_loss'], label='Validation')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "    # plt.show()  # Comentado para no mostrar la figura\n",
    "\n",
    "def show_confusion_matrix(y_true, y_pred, classes):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sp = plt.subplot(1, 1, 1)\n",
    "    ctx = sp.matshow(cm)\n",
    "    plt.xticks(list(range(len(classes))), labels=classes)\n",
    "    plt.yticks(list(range(len(classes))), labels=classes)\n",
    "    plt.colorbar(ctx)\n",
    "    # plt.show()  # Comentado para no mostrar la figura\n",
    "\n",
    "print('Usando TensorFlow versión', tf.__version__)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"dair-ai/emotion\")\n",
    "\n",
    "train = dataset['train']\n",
    "val = dataset['validation']\n",
    "test = dataset['test']\n",
    "\n",
    "def get_tweet(data):\n",
    "    tweets = [x['text'] for x in data]\n",
    "    labels = [x['label'] for x in data]\n",
    "    return tweets, labels\n",
    "\n",
    "tweets, labels = get_tweet(train)\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "lengths = [len(t.split(' ')) for t in tweets]\n",
    "plt.hist(lengths, bins=len(set(lengths)))\n",
    "maxlen = 50\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def get_sequences(tokenizer, tweets):\n",
    "    sequences = tokenizer.texts_to_sequences(tweets)\n",
    "    padded = pad_sequences(sequences, truncating='post', padding='post', maxlen=maxlen)\n",
    "    return padded\n",
    "\n",
    "padded_train_seq = get_sequences(tokenizer, tweets)\n",
    "\n",
    "classes = set(labels)\n",
    "print(classes)\n",
    "\n",
    "class_to_index = dict((c, i) for i, c in enumerate(classes))\n",
    "index_to_class = dict((v, k) for k, v in class_to_index.items())\n",
    "\n",
    "names_to_ids = lambda labels: np.array([class_to_index.get(x) for x in labels])\n",
    "\n",
    "train_labels = names_to_ids(labels)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 16, input_length=maxlen),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),\n",
    "    tf.keras.layers.Dense(len(classes), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "val_tweets, val_labels = get_tweet(val)\n",
    "val_seq = get_sequences(tokenizer, val_tweets)\n",
    "val_labels = names_to_ids(val_labels)\n",
    "\n",
    "h = model.fit(\n",
    "    padded_train_seq,\n",
    "    train_labels,\n",
    "    validation_data=(val_seq, val_labels),\n",
    "    epochs=20,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# show_history(h)  # Comentado para no mostrar la figura\n",
    "\n",
    "test_tweets, test_labels = get_tweet(test)\n",
    "test_seq = get_sequences(tokenizer, test_tweets)\n",
    "test_labels = names_to_ids(test_labels)\n",
    "\n",
    "_ = model.evaluate(test_seq, test_labels)\n",
    "\n",
    "i = random.randint(0, len(test_labels) - 1)\n",
    "print('Frase:', test_tweets[i])\n",
    "print('Emoción:', index_to_class[test_labels[i]])\n",
    "\n",
    "p = model.predict(np.expand_dims(test_seq[i], axis=0))[0]\n",
    "pred_class = index_to_class[np.argmax(p).astype('uint8')]\n",
    "print('Emoción Predicha:', pred_class)\n",
    "\n",
    "preds = model.predict(test_seq)\n",
    "classes_x = np.argmax(preds, axis=1)\n",
    "\n",
    "# show_confusion_matrix(test_labels, classes_x, list(classes))  # Comentado para no mostrar la figura\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se pudo traducir el texto: ¡A veces la productividad puede ser abrumadora, pero no nos desanimemos! ¡Es importante recordar que todos los obstáculos tienen solucion. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: ¡La procrastinación es uno de los peores enemigos de la productividad! No dejes que tu desánimo te impida lograr tus metas. ¡Esfu. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: \"A veces la frustración es parte de la productividad, pero hay que aprender a lidiar con ella para no dejarnos llevar por la negatividad\".. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: Sentirse desalentado y sin motivación es parte de la vida, pero nunca pierdas de vista tu objetivo final y sigue trabajando para mejorar tu. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: A veces es difícil ser productivo cuando nos sentimos abrumados por los sentimientos negativos. Pero hay que seguir adelante para alcanzar nuestras. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: Todo el esfuerzo que pongamos en ser más productivos puede ser en vano si no nos sentimos motivados y positivos. Dejemos que la emoción. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: \"La pereza es una de las mayores enemigas de la productividad. ¡Motívate y haz que tu energía se vuelva tu mejor aliada!\". Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: \"No hay nada peor que perder la motivación y la productividad por culpa de los sentimientos negativos. #Productividad\".. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: La productividad no siempre está acompañada de sentimientos positivos. A veces, la presión de lograr los objetivos puede generar desánimo. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: \"Somos responsables de nuestra propia productividad, pero a veces los sentimientos negativos nos impiden alcanzar nuestros objetivos. ¡Es hor. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: No desistas cuando te sientas abrumado por tus metas de productividad. Si te sientes desmotivado, toma un descanso para recuperar energía y focalizar. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: La productividad se ve afectada cuando estamos sumergidos en un estado de ánimo negativo. Debemos tomar el tiempo para identificar y liberar nu. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: A veces el cansancio y la desmotivación parecen demasiado pesados para seguir adelante con la productividad. ¡Es importante tomarse un descanso. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: No es fácil mantener la productividad cuando los sentimientos negativos te abruman. Pero hay que seguir adelante para alcanzar el éxito. #Product. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: \"La productividad es difícil cuando estás sintiendo emociones y sentimientos negativos. Es importante reconocerlo para buscar soluciones y as. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: A veces la productividad se ve obstaculizada por los sentimientos negativos. Es importante tomarse un tiempo para procesar tus emociones y encont. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: No hay nada peor que la productividad sin emoción. Sentirte abrumado, frustrado o desanimado puede hacer que sea aún más difícil alcan. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: A veces la productividad puede ser difícil de lograr cuando nos sentimos abrumados por emociones y sentimientos negativos. #Productividad. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: \"A veces la productividad puede ser abrumadora y desalentadora, pero es importante trabajar para encontrar la motivación para seguir adelante\".. Error: HTTP Error 400: Bad Request\n",
      "No se pudo traducir el texto: ¡A veces la productividad parece imposible cuando el sentimiento negativo te domina! Invierte tiempo en cuidar de ti mismo para llegar a. Error: HTTP Error 400: Bad Request\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def traducir_a_ingles(texto):\n",
    "    try:\n",
    "        blob = TextBlob(texto)\n",
    "        if blob.detect_language() != 'en':\n",
    "            texto = blob.translate(to='en').string\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo traducir el texto: {texto}. Error: {str(e)}\")\n",
    "    \n",
    "    return texto\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(traducir_a_ingles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..\\csv\\Ari_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traducir_a_ingles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..\\csv\\Mati_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "                                                tweet  emotion\n",
      "0   Sad news for those seeking joy: the search for...        0\n",
      "1   Devastating news of more job losses today, but...        0\n",
      "2   Sad news today, but we must keep a positive at...        0\n",
      "3   Sad news: another tragedy has hit the world to...        3\n",
      "4   Sometimes it's hard to find inspiration when s...        0\n",
      "5   Sometimes it can be hard to find motivation an...        0\n",
      "6   The path to inspiration can be long and windin...        3\n",
      "7   Sometimes it feels like no matter what you do,...        0\n",
      "8   Sometimes it can be hard to find the motivatio...        0\n",
      "9   Sometimes it feels like inspiration has abando...        0\n",
      "10  Sometimes it feels like inspiration is so far ...        1\n",
      "11  Sometimes it's hard to find inspiration when f...        1\n",
      "12  Sometimes it's hard to find inspiration when y...        0\n",
      "13  Sometimes it's hard to find inspiration when y...        0\n",
      "14  Sometimes it feels like we're stuck in a rut, ...        4\n",
      "15  Sometimes life can be overwhelming and it can ...        1\n",
      "16  Sometimes it's hard to find inspiration when y...        0\n",
      "17  Sometimes it can be difficult to find the moti...        0\n",
      "18  Sometimes we all need a little inspiration to ...        0\n"
     ]
    }
   ],
   "source": [
    "# Supongamos que tienes un DataFrame llamado df con una columna llamada \"tweet\"\n",
    "tweets = df['tweet'].tolist()\n",
    "\n",
    "# Obtén las secuencias de los tweets utilizando el tokenizer\n",
    "tweet_sequences = get_sequences(tokenizer, tweets)\n",
    "\n",
    "# Realiza las predicciones de las emociones\n",
    "predictions = model.predict(tweet_sequences)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Mapea los índices de las emociones predichas a las etiquetas correspondientes\n",
    "predicted_emotions = [index_to_class[label] for label in predicted_labels]\n",
    "\n",
    "# Agrega una nueva columna \"emotion\" al DataFrame con las emociones predichas\n",
    "df['emotion'] = predicted_emotions\n",
    "\n",
    "# Muestra el DataFrame con las emociones predichas\n",
    "print(df[['tweet', 'emotion']])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..\\csv\\Ari_Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Cargamos el dataframe\n",
    "df = pd.read_csv('csv\\Mati_tweets.csv')\n",
    "\n",
    "# Función para limpiar el texto del tweet\n",
    "def clean_text(text):\n",
    "    # Eliminamos las menciones, los enlaces y los caracteres especiales\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # Eliminamos las menciones\n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+', '', text) # Eliminamos los enlaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9ñáéíóúüÁÉÍÓÚÜ\\s]', '', text) # Eliminamos los caracteres especiales\n",
    "    return text\n",
    "\n",
    "# Función para traducir el texto al inglés y luego analizar la emoción\n",
    "def detect_emotion(text):\n",
    "    # Limpiamos el texto\n",
    "    text_cleaned = clean_text(text)\n",
    "    #si el texto no esta en español que haga esto\n",
    "    if not langid.classify(text_cleaned)[0] == 'en':\n",
    "        text_cleaned = GoogleTranslator(source='auto', target='english').translate(text_cleaned)\n",
    "    else:\n",
    "        text_cleaned = text_cleaned\n",
    "    # Analizamos la emoción en el texto traducido\n",
    "    emotion = nlp(text_cleaned)[0]['label']\n",
    "    return emotion\n",
    "\n",
    "# Cargamos el modelo de detección de emociones\n",
    "nlp = pipeline('sentiment-analysis', model='finiteautomata/beto-emotion-analysis')\n",
    "\n",
    "# Aplicamos la función a cada tweet\n",
    "df['emotion'] = df['tweet'].apply(detect_emotion)\n",
    "\n",
    "# Imprimimos los primeros registros para verificar\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perception_tagger: Package\n",
      "[nltk_data]     'averaged_perception_tagger' not found in index\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('averaged_perception_tagger')\n",
    "\n",
    "def identificar_tema_principal(dataframe, idioma):\n",
    "    # Obtener todos los tweets en una lista\n",
    "    tweets = dataframe['tweet'].tolist()\n",
    "\n",
    "    # Seleccionar stopwords según el idioma\n",
    "    if idioma == \"alemán\":\n",
    "        stop_words = set(stopwords.words('german'))\n",
    "    elif idioma == \"español\":\n",
    "        stop_words = set(stopwords.words('spanish'))\n",
    "    elif idioma == \"francés\":\n",
    "        stop_words = set(stopwords.words('french'))\n",
    "    elif idioma == \"inglés\":\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    elif idioma == \"italiano\":\n",
    "        stop_words = set(stopwords.words('italian'))\n",
    "    else:\n",
    "        raise ValueError(\"Idioma no soportado\")\n",
    "\n",
    "    # Tokenización de palabras y eliminación de stopwords\n",
    "    palabras = [word for tweet in tweets for word in word_tokenize(tweet.lower()) if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    # Etiquetado de partes del discurso\n",
    "    tagged_words = pos_tag(palabras)\n",
    "\n",
    "    # Contar la frecuencia de cada palabra\n",
    "    frecuencia_palabras = Counter(tagged_words)\n",
    "\n",
    "    # Ordenar las palabras por frecuencia\n",
    "    palabras_ordenadas = sorted(frecuencia_palabras.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Obtener el tema principal como la palabra más frecuente con etiqueta de sustantivo\n",
    "    tema_principal = None\n",
    "    for palabra, frecuencia in palabras_ordenadas:\n",
    "        if 'NN' in palabra[1]:  # Verificar si la palabra tiene etiqueta de sustantivo\n",
    "            tema_principal = palabra[0]\n",
    "            break\n",
    "\n",
    "    return tema_principal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\matia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\matia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\matia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'productividad'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identificar_tema_principal(df, 'español')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ari</td>\n",
       "      <td>¡A veces la productividad puede ser abrumadora...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ari</td>\n",
       "      <td>¡La procrastinación es uno de los peores enemi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ari</td>\n",
       "      <td>\"A veces la frustración es parte de la product...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ari</td>\n",
       "      <td>Sentirse desalentado y sin motivación es parte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ari</td>\n",
       "      <td>A veces es difícil ser productivo cuando nos s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ari</td>\n",
       "      <td>Todo el esfuerzo que pongamos en ser más produ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ari</td>\n",
       "      <td>\"La pereza es una de las mayores enemigas de l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ari</td>\n",
       "      <td>\"No hay nada peor que perder la motivación y l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ari</td>\n",
       "      <td>La productividad no siempre está acompañada de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ari</td>\n",
       "      <td>\"Somos responsables de nuestra propia producti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ari</td>\n",
       "      <td>No desistas cuando te sientas abrumado por tus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ari</td>\n",
       "      <td>La productividad se ve afectada cuando estamos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ari</td>\n",
       "      <td>A veces el cansancio y la desmotivación parece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ari</td>\n",
       "      <td>No es fácil mantener la productividad cuando l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ari</td>\n",
       "      <td>\"La productividad es difícil cuando estás sint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ari</td>\n",
       "      <td>A veces la productividad se ve obstaculizada p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ari</td>\n",
       "      <td>No hay nada peor que la productividad sin emoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ari</td>\n",
       "      <td>A veces la productividad puede ser difícil de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ari</td>\n",
       "      <td>\"A veces la productividad puede ser abrumadora...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ari</td>\n",
       "      <td>¡A veces la productividad parece imposible cua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                                              tweet\n",
       "0   Ari  ¡A veces la productividad puede ser abrumadora...\n",
       "1   Ari  ¡La procrastinación es uno de los peores enemi...\n",
       "2   Ari  \"A veces la frustración es parte de la product...\n",
       "3   Ari  Sentirse desalentado y sin motivación es parte...\n",
       "4   Ari  A veces es difícil ser productivo cuando nos s...\n",
       "5   Ari  Todo el esfuerzo que pongamos en ser más produ...\n",
       "6   Ari  \"La pereza es una de las mayores enemigas de l...\n",
       "7   Ari  \"No hay nada peor que perder la motivación y l...\n",
       "8   Ari  La productividad no siempre está acompañada de...\n",
       "9   Ari  \"Somos responsables de nuestra propia producti...\n",
       "10  Ari  No desistas cuando te sientas abrumado por tus...\n",
       "11  Ari  La productividad se ve afectada cuando estamos...\n",
       "12  Ari  A veces el cansancio y la desmotivación parece...\n",
       "13  Ari  No es fácil mantener la productividad cuando l...\n",
       "14  Ari  \"La productividad es difícil cuando estás sint...\n",
       "15  Ari  A veces la productividad se ve obstaculizada p...\n",
       "16  Ari  No hay nada peor que la productividad sin emoc...\n",
       "17  Ari  A veces la productividad puede ser difícil de ...\n",
       "18  Ari  \"A veces la productividad puede ser abrumadora...\n",
       "19  Ari  ¡A veces la productividad parece imposible cua..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
